ğŸ§© Dcluttr Data Analyst Task
ğŸ“Œ Objective- Create a derived table blinkit_city_insights integrating data from Blinkitâ€™s SKU-level inventory streams, category mapping, and city mapping tables to estimate quantity sold, sales, and stock metrics per SKU, city, and date.

ğŸ§  Problem Context-
E-commerce platforms rely on accurate inventory and sales insights.
Using Blinkitâ€™s raw datasets, this task focuses on estimating demand and tracking stock movement across cities and categories.

âš™ï¸ Skills & Tools-
SQL (MySQL) â€“ Joins, Window Functions, Aggregations, Subqueries, Indexing
Concepts: Inventory Movement, Mode Calculation, Discount Estimation, Data Normalization

ğŸ“Š Approach-
Data Setup â€“ Created and indexed base tables (scraping_stream, categories, city_map) and loaded CSVs.
Inventory Movement â€“ Used LAG() to compute changes between time slots â†’ est_qty_sold.
Mapping â€“ Joined with city and category data using CRC32() for city identifiers.
Price & Sales Estimation â€“ Derived SP/MRP modes and computed est_sales_sp, est_sales_mrp.
Metrics Calculated â€“ wt_osa, avg_discount, listed_ds_count, in_stock_ds_count.
Final Output â€“ Populated blinkit_city_insights table at grain: Date Ã— SKU Ã— City.

ğŸ’¼ Deliverables-
Task 1&2.sql â†’ Complete SQL pipeline
output.csv â†’ Final dataset (estimates & metrics)

ğŸ” Highlights-
Built end-to-end SQL workflow for city-level SKU insights.
Applied window functions and aggregations efficiently with indexing.
Generated scalable, analysis-ready dataset for sales and stock tracking.
